<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Deepdrive FAQ</title>

    <!-- Bootstrap -->
    <link href="vendor/css/bootstrap.min.css" rel="stylesheet">
    <style>body {
        background: #eee;
    }</style>
    <link href="vendor/css/docs.css" rel="stylesheet">
    <link href="assets/css/deepdrive.css" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body>

<div class="bg-small">
    <a href="index.html"><img class="bg-small-img" src="assets/img/logo_dark.svg" alt="Deepdrive logo"></a>
</div>

<!-- Docs page layout -->
<div class="bs-docs-header" id="content">
    <div class="container home">
        <h2 class="faq">General FAQ</h2>
        <div class="section">
            <ul>
                <li><b>What makes Deepdrive different than simulator / self-driving car company X?</b>
                    <div class="v-spacer">

                        <div>
                            Current simulators seem to tie themselves to specific hardware or do not have a path towards
                            deploying AI into physical vehicles.
                            We want to be hardware agnostic and deploy into real vehicles.
                        </div>
                        <div style="font-size: 0.3em">&nbsp;</div>
                        <div>
                            To do this, we will train AIs on a wide range of sensors,
                            cars, and environments (which is easy in simulation) and
                            facilitate transfer into real cars via techniques like
                            <a href="https://arxiv.org/abs/1711.03213">domain</a>
                            <a href="https://sites.google.com/view/graspgan">adaptation</a>.
                            This will allow
                            a larger number of people to iterate on production AI, using just the simulator,
                            while providing constant on-road testing of the best in-sim AIs in physical vehicles.
                        </div>
                        <div style="font-size: 0.3em">&nbsp;</div>
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>How close are autonomous vehicles to human-level driving?</b>
                    <div class="v-spacer">
                        Autonomous vehicles still underperform humans by a wide margin
                        at <a href="https://www.wired.com/story/self-driving-cars-disengagement-reports/">one human
                        intervention per 5k miles</a> - a figure which improved by just 10% from 2016 to 2017.
                        While ready-at-the-wheel drivers and
                        <a href="https://www.reuters.com/article/us-autos-autonomous-california/self-driving-cars-with-remote-drivers-could-test-on-calif-roads-in-april-dmv-idUSKCN1G72CU"
                            >remote operators</a> make self-driving a
                        <a href="https://www.theverge.com/2017/1/19/14326258/teslas-crash-rate-dropped-40-percent-after-autopilot-was-installed-feds-say"
                        >viable technology today</a>,
                        we believe that the right type of simulator can drastically accelerate progress
                        by giving more people the opportunity to contribute, allowing them to do so in software-only,
                        and by creating more fluid competition
                        between approaches.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>How will you grow?</b>
                    <div class="v-spacer">
                        Initially we are seeking funding through donations and sponsorship of self-driving competitions.
                        Our long-term goal is to create
                        a marketplace where AIs can be rated, reviewed, tested, and purchased.
                        Crowdsourced testing of AI in the marketplace will provide additional
                        transparency and opportunity for people to
                        participate in AI development.
                        Non-commercial use will always be free and our source will always be
                        available, so that researchers, individuals, and hobbyists
                        can use the simulator to evaluate their AI against the same benchmarks industry uses.
                    </div>
                </li>
                <li><b>What happened to the GTA-based version of Deepdrive?</b>
                    <div class="v-spacer">
                        Unfortunately, we were unable to license GTA V in a way that allowed us to distribute our
                        previous versions, including <a
                            href="https://www.reddit.com/r/SelfDrivingCars/comments/4wu7bd/someone_built_selfdriving_car_ai_that_lives/"
                        >Deepdrive 1.0</a> and
                        <a href="https://news.ycombinator.com/item?id=13375543">Deepdrive Universe</a>.

                        We've now rebuilt everything on Unreal Engine, and with its
                        full access to the engine source code, a great graphical editor,
                        and the general ability to modify any part of the simulation,
                        are elated about what we can bring to you in terms of capability, hackability, and
                        transparency.
                    </div>
                </li>
                <li><b>How useful is the current simulator in making progress on self-driving?</b>
                    <div class="v-spacer">
                        We think that something akin to a MNIST for self-driving is extremely important
                        for beginners to get up and running easily and also for allowing researchers to quickly evaluate
                        new ideas. Specifically, we see great promise in the ability to try methods
                        like reinforcement learning in our initial Deepdrive 2.0 release, something that would be too dangerous
                        to experiment with in the real world.
                        We also know that matching the richness and variability of real-world driving
                        within the simulation will be one of the keys to our success,
                        and we will work aggressively towards creating the most true-to-life
                        driving simulation possible.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>Do you plan to deploy AI into real cars?</b>
                    <div class="v-spacer">
                        Definitely! We plan to test in physical cars early and often by integrating with
                        systems like <a href="https://comma.ai">Comma.ai's</a>
                        or
                        <a href="https://drivekit.polysync.io/">Polysync's DriveKit</a>. If you're interested in
                        partnering with us
                        on a hardware integration, please reach out at hw@deepdrive.io.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
            </ul>
        </div>
        <h2 class="faq">Technical FAQ</h2>
        <div class="section">
            <ul>
                <li><b>Will you support Macs?</b>
                    <div class="v-spacer">Mac support requires one of two things, a shared memory implementation or
                        sending capture data over sockets.
                        The latter would be slower but would enable better distributed processing on all platforms.
                        Apple is <a href="https://developer.apple.com/development-kit/external-graphics/">incubating</a>
                        external GPU support which will bring MacOS on par with Linux and Windows in terms of
                        single-machine deep learning setups.
                        The simulation itself already runs on Mac, so it’s just a matter of sending and processing the
                        data from the environment with sharedmem/sockets and GPU accelerated ML respectively.
                        Slower-than-realtime support will be possible when we add synchronous stepping (see next
                        FAQ).
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>Is the API synchronous or asynchronous?</b>
                    <div class="v-spacer">
                        The agent and environment currently operate asynchronously from each
                        other except for resets and registering cameras.
                        So if the environment's frame rate drops below the agent's step rate, the agent will
                        receive blank
                        frames from the environment, and if the opposite happens, frames will be skipped. Existing RL
                        <a href="https://github.com/openai/baselines">baselines</a> like A2C use synchronized
                        stepping
                        across multiple environments through
                        <a href="https://github.com/openai/baselines/blob/3f676f7d1e16a0880cb7fed5850bc00efba413d6/baselines/common/vec_env/__init__.py">
                            vectorized environments</a>
                        which we will support in v3. Also, thanks to
                        the <a href="http://carla.org">Carla.org</a>
                        team for their technical guidance on ways to do synchronized stepping within Unreal.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>Why didn't you use sockets to transmit sensors?</b>
                    <div class="v-spacer">
                        Sending sensor data via sockets was a bottleneck in our testing.
                        Getting 8 HD cameras and depth at 60FPS, uncompressed back to the CPU gets too close
                        to the limits of what, even highly tuned, sockets can do. We are
                        still working on other
                        bottlenecks (like sequential rendering of cameras), but it’s important, and possible to
                        simulate a modern self-driving
                        stack in real-time on one machine - and we will stay committed
                        to doing that so developers with
                        one machine can test their agents.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>What is Deepdrive's sensor latency and throughput?</b>
                    <div>On a GTX 980, we currently see 20 fps with eight 512x512 60°FOV uncompressed
                        48bit color, 16bit depth cameras, 50 fps with one 512x512 camera,
                        and 10 fps with six 1920x1200 cameras. This includes copying and marshalling data
                        into NumPy arrays.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
                <li><b>How was the baseline agent trained?</b>
                    <div class="v-spacer">The baseline agent was trained on 8.2 hours of driving data
                        using a variant of <a href="https://youtu.be/C_LGsoe36I8?t=21m15s">DAgger</a>
                        where the human labels are replaced by an oracle path follower (defined in Car.cpp). See
                        tensorflow_agent/agent.py for exactly how we collect data.
                        Starting from BVLC alexnet pretrained weights, we
                        fine tuned for nearly 8 hours on a GTX 980. You can view our complete Tensorboard events
                        <a href="https://d1y4edi1yk5yok.cloudfront.net/tensorflow/baseline_tensorflow_train_and_eval.zip"
                        >here</a>. N.B. we needed to mix Linux and Windows collection in order to have the agent
                        perform well on both platforms (we used 80/20% Windows/Linux). Similarly,
                        we mixed data rendered in the Unreal Editor with data rendered in the packaged version
                        of the sim to get equal performance in both places.
                        We think this is due to rendering differences between
                        OpenGL, DirectX, and rendering optimizations made during packaging which
                        points to the importance of using capture augmentation in addition to using
                        domain adaptation techniques to increase transferability.
                    </div>
                    <div style="font-size: 0.3em">&nbsp;</div>
                </li>
            </ul>
        </div>
    </div>
    <div>&nbsp;</div>
    <div>&nbsp;</div>
    <div>&nbsp;</div>
</div>
<div class="container bs-docs-container blurred-body">

    <!-- Begin MailChimp Signup Form -->
    <div id="mc_embed_signup">
        <form
                action="//aiworld.us8.list-manage.com/subscribe/post?u=f9bbf4b0e7bc5e55647b6fb3c&amp;id=6c2b1fdb01"
                method="post" id="mc-embedded-subscribe-form"
                name="mc-embedded-subscribe-form" class="validate" target="_blank"
                novalidate>
            <div id="mc_embed_signup_scroll">
                <h2 class="subscribe">Subscribe to our mailing list</h2>
                <div class="mc-field-group">
                    <input placeholder="Email Address" type="email" value="" name="EMAIL"
                           class="required email" id="mce-EMAIL">
                </div>
                <div class="response" id="mce-responses" class="clear">
                    <div class="response" id="mce-error-response"
                         style="display:none"></div>
                    <div class="response" id="mce-success-response"
                         style="display:none"></div>
                </div>
                <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
                <div class="marvin" style="position: absolute; left: -5000px;"
                     aria-hidden="true"><input type="text"
                                               name="b_f9bbf4b0e7bc5e55647b6fb3c_6c2b1fdb01"
                                               tabindex="-1" value=""></div>

                <div class="submit"><input type="submit" value="Subscribe"
                                           name="subscribe"
                                           id="mc-embedded-subscribe"
                                           class="button"></div>
            </div>
        </form>
    </div>

    <!--End mc_embed_signup-->
    Made with <span class="heart">♡</span> on <a class="aiworld" href="https://www.youtube.com/watch?v=wupToqz1e2g">our pale blue dot</a>
</div>
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="vendor/js/bootstrap.min.js"></script>
<script src="vendor/js/docs.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<script src="vendor/js/ie10-viewport-bug-workaround.js"></script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-3213633-19', 'auto');
    ga('send', 'pageview');

</script>
</body>
</html>
