<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Rebuilding Deepdrive in Unreal</title>
    <meta property="og:image" content="https://deepdrive.io/assets/img/cam-depth3.jpg"/>
    <meta name="description" content="Deepdrive has been rebuilt without GTA using Unreal Engine.">

    <link href="../vendor/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {background: #eee;}
        .container {padding: 0;}
    </style>
    <link href="../vendor/css/docs.css" rel="stylesheet">
    <link href="../assets/css/deepdrive.css" rel="stylesheet">
    <link href="../assets/css/unreal-blog.css" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body>


<div class="bg-small">
    <a href="../index.html"><img class="bg-small-img" src="../assets/img/logo_dark.svg" alt="Deepdrive logo"></a>
</div>

<!-- Docs page layout -->
<div class="bs-docs-header" id="content">
    <div class="container home">
        <div class="blog-section">
            <div class="blog-title">Rebuilding Deepdrive on Unreal Engine</div>
            <div class="blog-time">April 12, 2018</div>
            <div class="blog-body">
                <img style="margin-bottom: 20px; width: 100%;" src="../assets/img/deepdrive-closeup3-min.jpg"/>
                <p>TLDR; Deepdrive has been rebuilt without GTA using Unreal Engine.
                    <a href="https://deepdrive.io">Check it out!</a>
                </p>
                <h2>Background - <a class="skip-link" href="#unreal">or skip to Unreal stuff</a></h2>
                <p>In early 2017, I worked with OpenAI to release an integration for GTAV into
                    the Universe platform. The goal was to make one of the most complex video games available to
                    researchers to experiment with methods like
                    <a href="http://rll.berkeley.edu/deeprlcourse/">deep reinforcement learning</a>
                    for self-driving.
                </p>

                <p>
                    Unfortunately, we had to shut the project down as TakeTwo was not
                    <a href="https://www.forbes.com/sites/aarontilley/2017/10/04/grand-theft-auto-v-the-rise-and-fall-of-the-diy-self-driving-car-lab/"
                    >receptive</a>
                    to people using GTAV for this purpose.
                </p>

                <div class="kibosh">
                    <img src="../assets/img/kibosh.jpg" />

                    <div>Deepdrive was taken down early 2017 due to legal issues around using GTAV.
                        Image from Deepdrive Universe blog post.</div>
                </div>


                <p>I know that many of you had spent time with Deepdrive 1.0 and Deepdrive Universe, and am
                    deeply sorry for not being able to continue to support your efforts.
                    I hope that rebuilding the project on Unreal will be able to restore what you
                    found valuable in the GTA version, and eventually do much more.
                </p>
                <p>Before the Universe integration had launched, I started work with the perception team at Uber's
                    Advanced Technologies Group.
                    Deepdrive caught their eye as relevant experience applying deep learning to self-driving. It was
                    also an opportunity to incorporate real world concerns back into Deepdrive, something I was grateful
                    to Uber for supporting.
                </p>
                <div class="levine-trajectory">
                    <a href="../assets/img/levine-trajectory-iid.jpg">
                        <img src="../assets/img/levine-trajectory-iid.jpg" align="left" /></a>
                    <br>
                    Levine's Fall 2017 Deep RL course
                    <a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_2_behavior_cloning.pdf"
                    >slides</a>
                </div>
                <div style="clear: right;"></div>
                <p style="margin-top: 100px;">
                    I noticed early on that the problems in simulation and in the real-world were often similar,
                    but the solutions very different.
                    For example,
                    in imitation learning, you often need to
                    reduce sampling error when training and test data
                    <a href="https://youtu.be/C_LGsoe36I8?t=14m31s">are not independently and identically distributed.</a>
                </p>
                <div style="clear: left;"></div>
                <div class="nvidia-dave">
                    <a href="../assets/img/nvidia-dave2.jpg">
                        <img src="../assets/img/nvidia-dave2.jpg"/></a>

                    <br>
                    <a href="https://arxiv.org/abs/1604.07316">Bojarski et al. ‘16, NVIDIA</a>
                </div>
                <p style="margin-top: 100px;">
                    In real cars this
                    <a href="https://arxiv.org/abs/1604.07316">is dealt with</a>,
                    by "pretending" as if the left or right camera is in the
                    center of the car and generating a corresponding corrective steering action to get back
                    to the center.
                    <a href="http://www.cs.cmu.edu/~seitz/papers/thesis.pdf">Viewpoint transformation</a>
                    then can interpolate between cameras to generate
                    additional corrective labels under the assumption of an infinite horizon - i.e. flat roads.
                </p>
                <div style="clear: left;"></div>
                <p>
                    In simulation, we can safely vary the trajectory of the car and see what an oracle
                    path-following agent does to correct its trajectory.
                </p>
                <div class="swerve-vid">
                    <iframe src="https://giphy.com/embed/2jMon1ulxOeNzZWk8H" width="480" height="270"
                            frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
                    <div>
                        Example of randomly varying the trajectory to reduce sampling error.
                        Frames captured during random actions are discarded, so
                        the agent is only trained on the course corrective behavior.
                    </div>

                </div>


                <div style="clear: both;"></div>
                <p>
                    So it’d be great to use simulated training in order to improve real-world performance as
                    we can get much more accurate corrective actions.
                    Excitingly, when we worked on verifying that agents can transfer between real and
                    virtual environments at Uber, I found that a network which has only seen real-world data could
                    drive in a simulated environment surprisingly well, i.e. stay in it's lane.
                    So going the other direction
                    is promising (virtual→real), although not something we were able to try.
                </p>

                <div style="clear: both; border-top: solid 1px #aaa; margin-bottom: 10px;"></div>


                <div class="pretrain">
                    <p class="pretrain-prose">
                        I want to provide a brief side note to how this is possible as it's quite promising.
                        Work from <a href="https://arxiv.org/abs/1504.00702">Finn, Levine et al.</a>,
                        demonstrates how you can train a neural network with ideal sensors, in their case
                        motion capture sensors, then swap out those sensors with a
                        network which has learned
                        to mimic them, allowing them to use solely the camera for input.

                        Applying this idea to self-driving, we could train an agent with any
                        state information available in the game (i.e. 3D or planning trajectories),
                        providing we can also learn to generate that information from sensors we'll have in the
                        real-world, i.e. cameras.

                        This allows us to use much more information during training, drawing from
                        essentially god-like sensors available in simulation.

                    </p>
                    <div class="pretrain-pose">
                        <a href="../assets/img/input-remapping-trick.jpg">
                            <img src="../assets/img/input-remapping-trick.jpg" align="left"/></a>
                        <div>
                            Using motion capture sensors for training a robot to act using just a camera.
                            <a href="https://arxiv.org/abs/1504.00702">Levine, Finn, et al. 16</a></div>
                    </div>
                    <div style="clear: both;"></div>

                    <p>
                        The trick then becomes making the simulation realistic enough for that training
                        to transfer to the real-world. This is where GANs and style transfer methods
                        could help generate contextually realistic data. Below are some visual examples.

                    </p>

                    <a href="https://junyanz.github.io/CycleGAN/"><img class="gen" src="../assets/img/cycle-gan.jpg" align="left"/></a>
                    <div><a href="https://junyanz.github.io/CycleGAN/">Cycle GAN</a> learns a bi-directional transformation between image classes</div>
                    <a href="https://github.com/luanfujun/deep-photo-styletransfer"><img class="gen" src="../assets/img/deep-photo-styletransfer.jpg" align="left"/></a>
                    <div style="margin-bottom: 10px;"><a href="https://junyanz.github.io/CycleGAN/">Deep photo style transfer</a> learns to stylize images with other images</div>

                    <p>
                        Okay, so this has become more than a brief side note, but the point is that
                        we can essentially cheat within simulation, and get away with it by using
                        neural networks which map the virtual world to the real world and vice versa.
                    </p>
                </div>



                <div style="clear: both; border-top: solid 1px #aaa; margin-bottom: 10px;"></div>
                <p>
                    Through experiences at Uber,
                    I became familiar with the problems of real autonomous vehicles -
                    digging into the fundamental reasons for our disengagements and riding along in test cars.
                    This gave me a deep appreciation for the difficultly of the problem
                    and made me increasingly excited about what an open self-driving simulator could to
                    help make self-driving happen.
                    <!--by-->
                    <!--<ol style="text-align: left; font-size: 17px;">-->
                    <!--<li >Letting more people work on the problem, and-->
                    <!--<li>Letting them quickly and safely try things in software without the immense amount of time,-->
                    <!--money, and risk required to do so on the road.-->
                    <!--</ol>-->
                <p>
                    So in November, I left to do just that and began work on Deepdrive 2.0.
                </p>
                <a class="skip-anchor" name="unreal"></a>
                <h2>The move to Unreal</h2>

                <div class="unreal-games">
                    <a href="../assets/img/gears4.jpg"><img width="300" src="../assets/img/gears4-min.jpg"/></a><a
                        href="../assets/img/pubg2.jpg"><img
                        width="300" height="168" src="../assets/img/pubg2-min.jpg"/></a><a
                        href="../assets/img/fortnite.jpg"><img width="300" src="../assets/img/fortnite-min.jpg"/></a>
                    <div>
                        Some of the popular games made in Unreal - Gears 4, PUBG, and Fortnite from left to right.
                        PUBG and Fortnite
                        are currently the most popular online games with
                        <a href="http://store.steampowered.com/stats/">
                            2M</a>
                        and <a
                            href="https://www.pcgamesn.com/fortnite/fortnite-vs-pubg-map-players-graphics-gameplay-weapons-review#playercount"
                    >3M</a>
                        <i>concurrent</i> users
                        respectively (that's close to the number of concurrent users on Facebook!).
                    </div>
                </div>

                <p>
                    The first step in developing Deepdrive 2.0 was choosing an engine.
                    Of the many we evaluated, Unreal not only checked the boxes, but "felt" best to us while using it,
                    especially in Linux.
                    From a feature perspective Unreal is the only free, shared source, AAA engine available.
                    Having the source is critical as we're going to need to optimize 8+
                    camera rendering, run faster than real-time, and possibly swap out the physics engine.
                </p>

                <table class="engine-compare">
                    <tbody>
                    <tr class="engine-margin">
                        <td style="border:none; background: white;"></td>
                        <td>Source</td>
                        <td>Pricing</td>
                        <td>Linux</td>
                        <td>AAA</td>
                    </tr>
                    <tr>
                        <td class="engine-name">Unreal</td>
                        <td class="engine-okay">Shared</td>
                        <td class="engine-okay">Free with revenue cap, then 5%</td>
                        <td class="engine-best">⭐⭐⭐⭐</td>
                        <td class="engine-best">Yes</td>
                    </tr>
                    <tr>
                        <td class="engine-name">Unity</td>
                        <td class="engine-worse">Paid access only</td>
                        <td class="engine-okay">Free with revenue cap, then fixed monthly</td>
                        <td class="engine-best">⭐⭐⭐</td>
                        <td class="engine-best">Yes</td>
                    </tr>
                    <tr>
                        <td class="engine-name">LumberYard (CryEngine fork)</td>
                        <td class="engine-okay">Shared</td>
                        <td class="engine-best">Free</td>
                        <td class="engine-worst">No</td>
                        <td class="engine-best">Yes</td>
                    </tr>
                    <tr>
                        <td class="engine-name">CryEngine</td>
                        <td class="engine-worst">Simulation not permitted</td>
                        <td class="engine-okay">$9.90/mo</td>
                        <td class="engine-best">⭐⭐</td>
                        <td class="engine-best">Yes</td>
                    </tr>
                    <tr>
                        <td class="engine-name">Quake3 - ioq3</td>
                        <td class="engine-better">GPL-2</td>
                        <td class="engine-best">Free</td>
                        <td class="engine-best">⭐?</td>
                        <td class="engine-worst">No</td>
                    </tr>
                    </tbody>
                </table>
                <div class="engine-compare-caption">
                    Our comparison chart of game engines circa 2017. Quake 3 thrown in for its use by <a
                        href="https://github.com/deepmind/lab">
                    DeepMind</a>, a major driver of increasing sophistication of environments
                    <a href="https://scholar.google.com/scholar?cites=3142026830514187809&as_sdt=805&sciodt=0,3&hl=en"
                    >used in</a>
                    <a href="https://scholar.google.com/scholar?cites=10877906446132695264&as_sdt=805&sciodt=0,3&hl=en"
                    >major recent research</a>.
                </div>

                <p>
                    There are a lot of other engines out there, including some really interesting ones
                    like the MIT licensed <a href="https://godotengine.org/">Godot</a>, however when it comes to
                    realism - Unity, Unreal, CryEngine and are still quite far ahead of the pack.
                </p>


                <div style="margin-bottom: 30px; ">
                    <iframe style="width: 100%; height: 300px;" src="https://www.youtube.com/embed/ZRKHzZ6OeWE"
                            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    <br>
                    Example of the latest in realism.
                    Demo
                    <a href="https://venturebeat.com/2018/03/21/epic-games-shows-real-time-ray-tracing-in-unreal-engine-with-star-wars-demo/"
                    > at GDC 18</a> of real-time raytracing in Unreal.
                </div>

                <h2>Unreal's self-driving ecosystem</h2>
                <p>
                    Besides Deepdrive, open source self-driving simulation projects like
                    <a href="https://github.com/Microsoft/AirSim"
                    >AirSim</a>
                    and
                    <a href="http://carla.org"
                    >Carla</a>
                    are also using Unreal. It’s my hope these sims can reuse components
                    and cooperate. We have already worked with the Carla team on some synchronization functionality
                    and it’s very exciting to see the growing community around open self-driving.
                    In addition, the self-driving simulators at
                    <a href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DbooEg6iGNyo&sa=D&ust=1523169575334000"
                    >NVIDIA</a>,
                    <a href="https://www.linkedin.com/jobs/view/senior-software-engineer-unreal-engine-at-cruise-automation-586806592/"
                    >Cruise</a>, and
                    <a href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DotmxoK4lCNw%26feature%3Dyoutu.be%26t%3D23m15s&sa=D&ust=1523169575332000"
                    >Zoox</a> are all built on Unreal, and there are likely more who've not yet gone public about it.
                </p>


                <div style="clear: both;"></div>

                <div class="sim-col-1">

                    <a href="../assets/img/airsim.gif">
                        <iframe src="https://giphy.com/embed/5bocFGEFLEjoV2Ey7k" frameBorder="0"
                                class="airsim giphy-embed" allowFullScreen></iframe></a>

                    <br>
                    Microsoft AirSim
                    <br>
                    <br>
                    <iframe src="https://giphy.com/embed/7JawEBLMNRXGTApR5R" frameBorder="0" class="carla giphy-embed"
                            allowFullScreen></iframe>
                    <br>
                    Carla
                    <br>
                    <br>
                </div>
                <div class="sim-col-2">
                    <iframe src="https://giphy.com/embed/PhIm0z1On6o5XZYE0k" frameBorder="0" class="autosim giphy-embed"
                            allowFullScreen></iframe>
                    <br>
                    NVIDIA AutoSim
                    <br>
                    <br>
                    <iframe src="https://giphy.com/embed/1gdwLLb858DCdQKGWU" frameBorder="0"
                            class="zoox giphy-embed" allowFullScreen></iframe>
                    <br>
                    Zoox's simulator
                    <br>
                    <br>
                </div>

                <div style="clear: both;"></div>
                <div style="margin-bottom: 20px;">Self-driving simulators built in Unreal</div>


                <p>
                    Some of the unique things about Deepdrive that sets us apart from these other projects are

                    <ol style="text-align: left; font-size: 17.68px;">
                        <li>Our frame rates are higher while capturing several cameras, due to use of shared memory rather
                            than sockets and asynchronous transfer.</li>
                        <li>Our road surface is not flat, but hilly, curvy, and varies in width</li>
                        <li>Our assets - the map, cars, lighting, etc… are free, come with the project, and modifiable in
                            Unreal.</li>
                    </ol>
                </p>

                <p>
                    Regardless, I think it’s a benefit to everyone to have a variety of simulators available.
                    A cross-simulator
                    benchmark would be extremely helpful for helping test agents ability to transfer
                    between environments, including to the real world.
                    If you’re interested in working on this idea
                    <a href="mailto:craig@deepdrive.io?Subject=simulation%20transfer" target="_top"
                    >drop me a line</a>.
                </p>
                <div style="clear: both"></div>
                <h2>Developing in Unreal</h2>
                <p>
                    You don't need to develop in Unreal to use Deepdrive; you just need to write Python.
                    However, for many projects, it will be important to be able to modify the simulation,
                    or at least see how it works at its lowest levels.

                </p>
                <h3>Built-in vehicle support</h3>
                <div class="wheeled-vehicle">
                    <video controls loop poster="../assets/img/wheeled-vehicle.jpg">
                        <source src="../assets/video/wheeled-vehicle-unreal.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>


                <p class="wheeled-vehicle-prose">
                    For example, there are a plethora of options available in Unreal for changing the way the
                    <a href="https://docs.unrealengine.com/en-US/Engine/Physics/Vehicles/VehicleUserGuide"
                    >vehicle</a> moves, including torque and steering response, differential, transmission,
                    and engine settings.
                </p>

                <div style="clear: both;"></div>

                <h3>Visual scripting</h3>

                <div class="blueprint-vid">
                    <iframe
                            src="https://www.youtube.com/embed/lL5J_qbVxUk?rel=0&amp;showinfo=0" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    <br>
                    Adding a keybind to one of Unreal's built-in AI functions in a Blueprint.
                </div>

                <p>
                    You can also prototype ideas in Unreal very quickly thanks to blueprints, it's node-based scripting
                    environment. Blueprints are 10x slower at runtime than C++, but it's easy to detect hotspots with Unreal's
                    GPU and CPU profilers and reimplement slow Blueprint code in C++.
                </p>

                <div class="profiler">
                    <a href="../assets/img/cpu-profiler.jpg"><img src="../assets/img/cpu-profiler-min.jpg" /></a>
                    <a href="../assets/img/gpu-profiler.jpg"><img src="../assets/img/gpu-profiler-min.jpg"/></a>
                    <br>
                    Unreal CPU profiler and GPU profiler
                </div>

                <div style="clear:both"></div>

                <h3>Unreal console</h3>
                <div class="unreal-console">
                    <iframe src="https://giphy.com/embed/RJy4kBjPzZjm2Y23t1" frameBorder="0"
                            class="giphy-embed" allowFullScreen></iframe>
                    <div style="margin-bottom: 30px;">
                        Press <kbd>``</kbd> in Deepdrive to open the Unreal Console and call into C++.
                    </div>
                </div>

                <p style="margin-top: 40px; ">
                    We ship our sim with the Unreal Console enabled, meaning you can call
                    any of built-in Unreal commands straight from the game. You can also call your
                    own C++ the same way with custom commands.
                </p>


                <div style="clear:both"></div>
                <h3>Substance</h3>

                <div class="substance">
                    <a href="../assets/img/Substance_designer_spring_2018_SL_Header_PC_2000x850_0.jpg"><img
                            src="../assets/img/Substance_designer_spring_2018_SL_Header_PC_2000x850_0-min.jpg"/></a>
                </div>


                <p style="margin-top: 60px; ">
                    We use <a href="https://www.allegorithmic.com/">Substance</a>
                    and its Unreal integration for <a
                        href="https://github.com/deepdrive/deepdrive-sim/tree/master/Content/TuningCars/Materials/Hangar/material/Substance"
                >many of our materials,</a> which we have found to be the source of several incredibly realistic
                    surfaces in Unreal games. Substance also makes it easy to procedurally generate your own realistic materials
                    and textures.
                </p>

                <div style="clear: both"></div>

                <h3>Debugging workflow</h3>
                <div style="float: right; margin-left: 40px; margin-bottom: 50px;">
                    <a href="../assets/img/debugging.jpg"><img width="400" src="../assets/img/debugging-min.jpg"/></a>
                    <br>
                    Visual Studio debugging. CLion on Linux <a
                        href="https://blog.jetbrains.com/clion/2016/10/clion-and-ue4/">is also supported</a>.
                </div>


                <p style="margin-top: 65px;">
                    Unreal's debugging workflow allows you to easily break anywhere into
                    engine code or your own code, then hot-reload C++ in your project.
                </p>

                <div style="clear: both"></div>


                <!--<img width="400" src="../assets/img/cube-map.jpg"/>-->

                <h2>Conclusion</h2>
                <p>
                    So far, we've been very happy with Unreal Engine, a AAA game development platform
                    with accessible and modifiable source,
                    and a solid start at becoming the engine of choice for self-driving simulation.
                    We look forward to using it to make self-driving accessible to everybody.
                </p>

                <h2>Stepping back</h2>
                <p>
                    Creating fully autonomous cars will mean developing AI that has a
                    grounded understanding of physics, our 3D world, and how to interact, predict, and cooperate with
                    us.

                    These capabilities will enable AI to understand more abstract concepts
                    within language, read the web, and someday become more capable than humans at every job
                    there is.

                    It's therefore extremely important that self-driving AI be developed in the open
                    and in a manner sensitive to the sweeping effects AI will have.
                </p>
                <p>

                    <!--Open simulation allows developing AI-->
                    <!--in a way that is sensitive sweeping this transition will have, where-->
                    <!--ensuring safety, transparency, and broad participation-->

                    A beneficial distinction of self-driving vs other large existing AI applications (i.e. search and social)
                    is that the privacy and secrecy concerns are minimal. This enables open development and
                    many more people to work on the problem with a level of transparency not usually found in applied AI.

                    In general, developing AI applications in the open is important so that people have
                    access to the technology making an increasing number of decisions in their lives.
                </p>
                <p>

                    <!--Also, involving more people is good for both short-term safety, due to increased transparency and resources-->
                    <!--for development and testing, and long-term safety, where safety in the-->
                    <!--short-term derives from increased transparency and testing of self-driving, and safety in the long-term-->
                    <!--derives from larger numbers of people having access to technology that makes an increasing-->
                    <!--majority of important decisions within our world.-->

                                        <!--compared to other large AI applications (i.e. search and social),-->
                    <!--and that involving more people is good for both short-term safety, providing transparency and increased resources-->
                    <!--for development and testing, and long-term safety, ensuring AI is not controlled by relatively-->
                    <!--small numbers of people.-->


                    Deepdrive is a self-driving car simulation, but our goals extend beyond that.
                    Our aim is to create a platform that sets a precedent for the safe development
                    of future applied AI.
                    The type of AI which we can all be proud to say was - created by us.

                </p>
            </div>
        </div>
    </div>
</div>
<div class="container bs-docs-container blurred-body">

    <!-- Begin MailChimp Signup Form -->
    <div id="mc_embed_signup">
        <form
                action="//aiworld.us8.list-manage.com/subscribe/post?u=f9bbf4b0e7bc5e55647b6fb3c&amp;id=6c2b1fdb01"
                method="post" id="mc-embedded-subscribe-form"
                name="mc-embedded-subscribe-form" class="validate" target="_blank"
                novalidate>
            <div id="mc_embed_signup_scroll">
                <h2 class="subscribe">Subscribe to our mailing list</h2>
                <div class="mc-field-group">
                    <input placeholder="Email Address" type="email" value="" name="EMAIL"
                           class="required email" id="mce-EMAIL">
                </div>
                <div class="response" id="mce-responses" class="clear">
                    <div class="response" id="mce-error-response"
                         style="display:none"></div>
                    <div class="response" id="mce-success-response"
                         style="display:none"></div>
                </div>
                <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
                <div class="marvin" style="position: absolute; left: -5000px;"
                     aria-hidden="true"><input type="text"
                                               name="b_f9bbf4b0e7bc5e55647b6fb3c_6c2b1fdb01"
                                               tabindex="-1" value=""></div>

                <div class="submit"><input type="submit" value="Subscribe"
                                           name="subscribe"
                                           id="mc-embedded-subscribe"
                                           class="button"></div>
            </div>
        </form>
    </div>


    <!--End mc_embed_signup-->
    Made with <span class="heart">♡</span> on <a class="aiworld" href="https://www.youtube.com/watch?v=wupToqz1e2g">our
    pale blue dot</a>
</div>
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="../vendor/js/bootstrap.min.js"></script>
<script src="../vendor/js/docs.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<script src="../vendor/js/ie10-viewport-bug-workaround.js"></script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-3213633-19', 'auto');
    ga('send', 'pageview');

</script>
</body>
</html>
